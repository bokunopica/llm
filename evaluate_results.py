import json
import os
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
)


def load_jsonl(file_path):
    """åŠ è½½JSONLæ–‡ä»¶"""
    results = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                results.append(json.loads(line))
    return results


def normalize_label(label):
    """æ ‡å‡†åŒ–æ ‡ç­¾ï¼Œå°†æ‰€æœ‰å¯èƒ½çš„å˜ä½“è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼"""
    if isinstance(label, str):
        label = label.lower().strip()

        # æ£€æŸ¥æ˜¯å¦åŒ…å«malignantå…³é”®è¯
        if any(
            keyword in label
            for keyword in ["malignant", "cancer", "malicious", "suspicious"]
        ):
            return "malignant"

        # æ£€æŸ¥æ˜¯å¦åŒ…å«benignå…³é”®è¯
        if any(
            keyword in label
            for keyword in ["benign", "normal", "healthy", "non-malignant"]
        ):
            return "benign"

        # ç²¾ç¡®åŒ¹é…
        if label in ["malignant", "malignant.", "[malignant]"]:
            return "malignant"
        elif label in ["benign", "benign.", "[benign]"]:
            return "benign"

    return label


def extract_predictions_and_labels(results):
    """ä»ç»“æœä¸­æå–é¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾"""
    predictions = []
    labels = []

    for result in results:
        # æå–é¢„æµ‹ç»“æœ
        pred = result.get("response", "").strip()
        pred = normalize_label(pred)

        # æå–çœŸå®æ ‡ç­¾
        label = result.get("labels", "").strip()
        label = normalize_label(label)

        # åªæœ‰å½“é¢„æµ‹å’Œæ ‡ç­¾éƒ½æœ‰æ•ˆæ—¶æ‰æ·»åŠ 
        if pred in ["malignant", "benign"] and label in ["malignant", "benign"]:
            predictions.append(pred)
        else:
            predictions.append("benign")  # é»˜è®¤é¢„æµ‹ä¸ºbenign
            # print(f"Invalid prediction or label: {pred} | {label}")
        labels.append(label)

    return predictions, labels


def calculate_metrics(predictions, labels):
    """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
    # è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ‡ç­¾ (malignant=1, benign=0)
    y_true = [1 if label == "malignant" else 0 for label in labels]
    y_pred = [1 if pred == "malignant" else 0 for pred in predictions]

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average="binary")
    recall = recall_score(y_true, y_pred, average="binary")
    f1 = f1_score(y_true, y_pred, average="binary")

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)

    # è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    report = classification_report(y_true, y_pred, target_names=["benign", "malignant"], digits=5)

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1,
        "confusion_matrix": cm,
        "classification_report": report,
        "total_samples": len(y_true),
        "true_malignant": sum(y_true),
        "true_benign": len(y_true) - sum(y_true),
        "pred_malignant": sum(y_pred),
        "pred_benign": len(y_pred) - sum(y_pred),
    }


def calculate_class_specific_metrics(predictions, labels):
    """è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡"""
    # è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ‡ç­¾
    y_true = [1 if label == "malignant" else 0 for label in labels]
    y_pred = [1 if pred == "malignant" else 0 for pred in predictions]

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    # å¯¹äºMalignantç±»ï¼ˆé˜³æ€§ç±»ï¼‰
    malignant_precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    malignant_recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    malignant_f1 = (
        2
        * (malignant_precision * malignant_recall)
        / (malignant_precision + malignant_recall)
        if (malignant_precision + malignant_recall) > 0
        else 0
    )

    # å¯¹äºBenignç±»ï¼ˆé˜´æ€§ç±»ï¼‰
    benign_precision = tn / (tn + fn) if (tn + fn) > 0 else 0
    benign_recall = tn / (tn + fp) if (tn + fp) > 0 else 0
    benign_f1 = (
        2 * (benign_precision * benign_recall) / (benign_precision + benign_recall)
        if (benign_precision + benign_recall) > 0
        else 0
    )

    return {
        "malignant": {
            "precision": malignant_precision,
            "recall": malignant_recall,
            "f1_score": malignant_f1,
            "support": tp + fn,
        },
        "benign": {
            "precision": benign_precision,
            "recall": benign_recall,
            "f1_score": benign_f1,
            "support": tn + fp,
        },
        "confusion_matrix": {
            "true_negative": tn,
            "false_positive": fp,
            "false_negative": fn,
            "true_positive": tp,
        },
    }


# def print_results(metrics, class_metrics=None):
def print_results(metrics):
    """æ‰“å°ç»“æœ"""
    print("=" * 60)
    print("EVALUATION RESULTS")
    print("=" * 60)

    print(f"\nğŸ“Š OVERALL METRICS:")
    print(f"Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"Precision: {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)")
    print(f"Recall:    {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)")
    print(f"F1 Score:  {metrics['f1_score']:.4f} ({metrics['f1_score']*100:.2f}%)")

    # print(f"\nğŸ” CONFUSION MATRIX:")
    # cm = class_metrics["confusion_matrix"]
    # print(f"                 Predicted")
    # print(f"                 Benign  Malignant")
    # print(f"Actual  Benign   {cm['true_negative']:6d}  {cm['false_positive']:9d}")
    # print(f"        Malignant{cm['false_negative']:6d}  {cm['true_positive']:9d}")

    print(f"\nğŸ“‹ DETAILED CLASSIFICATION REPORT:")
    print(metrics["classification_report"])
    print(type(metrics["classification_report"]))


def eval(input_file):
    # åŠ è½½ç»“æœ

    print(f"Loading results from: {input_file}")
    results = load_jsonl(input_file)
    print(f"Loaded {len(results)} results")

    # æå–é¢„æµ‹å’Œæ ‡ç­¾
    predictions, labels = extract_predictions_and_labels(results)
    print(f"Valid predictions: {len(predictions)}")

    if len(predictions) == 0:
        print("âŒ No valid predictions found!")
        return

    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(predictions, labels)
    calculate_class_specific_metrics(predictions, labels)


    # æ‰“å°ç»“æœ
    # print_results(metrics, class_metrics)
    print_results(metrics)

    # # ä¿å­˜ç»“æœ
    # save_results(
    #     metrics,
    #     class_metrics,
    #     output_file=input_file.replace(".jsonl", "_evaluation.json"),
    # )


def main():
    import argparse

    argparse.parser = argparse.ArgumentParser(description="Evaluate inference results")
    argparse.parser.add_argument(
        "--input_file",
        type=str,
        required=True,
        help="Path to the input JSONL file containing inference results",
    )
    eval(argparse.parser.parse_args().input_file)


if __name__ == "__main__":
    main()
