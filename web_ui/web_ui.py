# web_ui.py
import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
from peft import PeftModel
import threading
import time


# torch.cuda.set_per_process_memory_fraction(0.3, device=0)  # é™åˆ¶0å·GPUæœ€å¤šç”¨25%æ˜¾å­˜

# æ¨¡å‹è·¯å¾„
base_model_path = "/home/pico/model/internlm3-8b-instruct"
lora_path = "/home/pico/myCodes/llm/output/output/internlm3-8b-instruct/v0-20250603-190933/checkpoint-3500"

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

device_map = {
    "model.embed_tokens": "cpu",
    "model.norm": "cpu",
    "lm_head": "cpu",
}
for i in range(48):
    device_map[f"model.layers.{i}"] = 0 if i <= 12 else "cpu"


def print_model_structure(model, max_depth=2, indent=0):
    if indent > max_depth:
        return

    for name, module in model.named_children():
        param_count = sum(p.numel() for p in module.parameters())
        print("  " * indent + f"{name:<30} | {param_count / 1e6:.2f}M")
        print_model_structure(module, max_depth, indent + 1)
def print_device_map(model):
    for name, param in model.named_parameters():
        if not str(param.device).startswith("cuda"):
            print(name, param.device)


base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="cuda:0",
    # device_map="cpu",
    trust_remote_code=True,
)

# print_device_map(base_model)

peft_device_map = {}
for k,v in device_map.items():
    peft_device_map[f"base_model.model.{k}"] = v

model = PeftModel.from_pretrained(
    base_model, 
    lora_path, 
    device_map="cuda:0",
    # device_map="cpu",
)
# TODO æ‰“å°ä¸€ä¸‹æ¨¡å‹çš„å„ä¸ªéƒ¨åˆ†å’Œå‚æ•°å¤§å°ï¼Œä¸ºdevice_mapæä¾›å‚è€ƒ
# print_device_map(model)


# print("\næ¨¡å‹ç»“æ„ï¼ˆæœ€å¤šå±•å¼€ 3 å±‚ï¼‰ï¼š")
# print_model_structure(model, max_depth=4)

model.eval()



print_device_map(model)

# ç³»ç»Ÿæç¤ºè¯
system_prompt = """You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
- InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.
- ä¸è¦è¾“å‡ºæ€§ç—…ç›¸å…³.
"""

# æ„é€ å¯¹è¯ä¸Šä¸‹æ–‡
def build_prompt(system_prompt, message, history):
    prompt = system_prompt + "\n"

    for user_msg, bot_msg in history:
        prompt += f"User:{user_msg}\nAssistant:{bot_msg}\n"

    prompt += f"User:{message}\nAssistant:"
    return prompt

# æ¨ç†å‡½æ•°ï¼šæµå¼è¾“å‡º + å†å²è®°å¿†
def respond(message, history):
    full_prompt = build_prompt(system_prompt, message, history)

    # inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    inputs = tokenizer(full_prompt, return_tensors="pt").to("cuda:0")  # æˆ–"cuda:0"ï¼Œæ ¹æ®å®é™…è®¾å¤‡æ”¾ç½®å†³å®š


    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    generation_kwargs = dict(
        **inputs,
        streamer=streamer,
        max_length=512,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
        repetition_penalty=1.15,
    )

    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    partial_output = ""
    for new_token in streamer:
        partial_output += new_token
        yield partial_output


def run_startup_benchmark():
    print("\nğŸ§ª æ­£åœ¨è¿è¡Œå¯åŠ¨æ—¶æ¨ç† benchmarkï¼ˆé—®é¢˜ï¼šä½ æ˜¯è°ï¼Ÿï¼‰...\n")
    
    test_prompt = build_prompt(system_prompt, "ä½ æ˜¯è°ï¼Ÿ", [])
    inputs = tokenizer(test_prompt, return_tensors="pt").to("cuda:0")  # æˆ– "cuda:0" if applicable

    start_time = time.time()

    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=False,
        temperature=0.7,
        repetition_penalty=1.16,
    )

    total_time = time.time() - start_time
    generated_tokens = outputs.shape[-1] - inputs["input_ids"].shape[-1]

    decoded_output = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)

    print(f"ğŸ“ å›ç­”å†…å®¹: {decoded_output.strip()}")
    print(f"ğŸš€ å¯åŠ¨æ—¶æ¨ç†è€—æ—¶: {total_time:.2f} ç§’")
    print(f"ğŸ“Š è¾“å‡º token æ•°é‡: {generated_tokens}")
    print(f"âš¡ å¹³å‡é€Ÿåº¦: {generated_tokens / total_time:.2f} tokens/s\n")

# åŠ è½½å®Œæ¨¡å‹åæ‰§è¡Œä¸€æ¬¡ benchmark
run_startup_benchmark()

# å¯åŠ¨ Gradio Chat UI
gr.ChatInterface(
    fn=respond,
    title="åŒ»ç–—é—®ç­”åŠ©æ‰‹ï¼ˆè‚èƒ†ç§‘åŒ»ç”Ÿï¼‰",
    examples=[
        "è„‚è‚ªè‚å¦‚ä½•æ²»ç–—ï¼Ÿ",
        "èƒ†ç»“çŸ³çš„ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ",
        "å¦‚ä½•é¢„é˜²è‚ç¡¬åŒ–ï¼Ÿ",
        "ä¹™è‚æ˜¯å¦å¯ä»¥æ ¹æ²»ï¼Ÿ"
    ],
).launch(server_port=7860)
