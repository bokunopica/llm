from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import pandas as pd
import torch
from torch.utils.data import Dataset
from PIL import Image
from transformers import AutoProcessor
import os
import pwd
from jinja2.exceptions import TemplateError
import traceback
import torch.nn.functional as F


# @dataclass
# class QaImageOutput:
#     q_input_ids: torch.Tensor
#     pixel_values: torch.Tensor
#     a_input_ids: torch.Tensor


class LlavaDataset(Dataset):
    def __init__(self, dataset_dir: str, is_train: bool = True) -> None:
        super().__init__()
        self.is_train = is_train
        self.chat_data, self.image_dir = self.build_dataset(dataset_dir)

    def build_dataset(self, data_dir: str) -> Tuple[List[Dict], Path]:
        data_dir = Path(data_dir)
        chat_file = data_dir.joinpath("train.jsonl" if self.is_train else "test.jsonl")
        image_dir = data_dir.joinpath("images_dl")
        print(f"Loading file: {chat_file}, exists: {chat_file.exists()}")
        chat_data = pd.read_json(chat_file, lines=True).to_dict(orient="records")
        return chat_data, image_dir

    def __len__(self):
        return len(self.chat_data)

    def __getitem__(self, index) -> Tuple[str, str, Path]:
        cur_data = self.chat_data[index]
        conversations = cur_data.get("conversations")
        human_input = conversations[0].get("value")
        chatbot_output = conversations[1].get("value")
        image_path = self.image_dir.joinpath(cur_data.get("image"))
        return human_input, chatbot_output, image_path


class LIDCClassificationDataset(Dataset):
    def __init__(
        self,
        dataset_dir: str,
        is_train: bool = True,
    ) -> None:
        super().__init__()
        self.is_train = is_train
        self.chat_data = self.build_dataset(dataset_dir)

    def build_dataset(self, data_dir: str) -> List[Dict]:
        data_dir = Path(data_dir)
        chat_file = data_dir.joinpath("train.jsonl" if self.is_train else "test.jsonl")
        print(chat_file)
        chat_data = pd.read_json(chat_file, lines=True).to_dict(orient="records")
        return chat_data

    def __len__(self):
        return len(self.chat_data)

    def __getitem__(self, index) -> Tuple[str, str, str]:
        cur_data = self.chat_data[index]
        human_input = "<image>\n" + cur_data.get("query", "")
        chatbot_output = cur_data.get("response")
        current_user = pwd.getpwuid(os.getuid()).pw_name
        image_path = cur_data.get("images")
        if current_user == "pico":
            image_path = image_path.replace("/home/qianq/", "/home/pico/")
        return human_input, chatbot_output, image_path


# def build_qaimage(
#     processor: AutoProcessor,
#     q_text: str,
#     a_text: str,
#     image_path: Path,
#     sys_prompt: str = "You are a helpful assistant.",
# ) -> QaImageOutput:
#     messages = [
#         {"role": "system", "content": sys_prompt},
#         {"role": "user", "content": q_text},
#     ]
#     try:
#         prompt = processor.tokenizer.apply_chat_template(
#             messages,
#             tokenize=False,
#             add_generation_prompt=True,
#         )
#     except TemplateError:
#         messages = [
#             {"role": "user", "content": sys_prompt + "\n" + q_text},
#         ]
#         prompt = processor.tokenizer.apply_chat_template(
#             messages,
#             tokenize=False,
#             add_generation_prompt=True,
#         )
#     except Exception:
#         prompt = f"{sys_prompt}\nUser: {q_text}\nAssistant:"

#     raw_image = Image.open(image_path).convert("RGB")
#     inputs = processor(raw_image, prompt, return_tensors="pt")
#     a_input_ids = processor.tokenizer(
#         a_text,
#         return_tensors="pt",
#         padding="longest",
#         truncation=True,
#         max_length=2048,
#     )["input_ids"]

#     return QaImageOutput(
#         q_input_ids=inputs.get("input_ids"),
#         pixel_values=inputs.get("pixel_values"),
#         a_input_ids=a_input_ids,
#     )


# class TrainLlavaModelCollator:
#     def __init__(
#         self,
#         processor: AutoProcessor,
#         ignore_index: int = -100,
#         system_prompt: Optional[str] = None,
#     ):
#         self.processor = processor
#         self.ignore_index = ignore_index
#         self.system_prompt = system_prompt or "You are a helpful assistant."
#         if not hasattr(self.processor, 'patch_size') or self.processor.patch_size is None:
#             self.processor.patch_size = 14  # LLaVA é€šå¸¸ä½¿ç”¨çš„ patch_size å€¼


#     def convert_sample(
#         self,
#         q_input_ids: torch.Tensor,
#         a_input_ids: torch.Tensor,
#     ) -> Tuple[torch.Tensor, torch.Tensor]:
#         eos_token_id = self.processor.tokenizer.eos_token_id
#         eos_tensor = torch.tensor([[eos_token_id]], dtype=torch.long)
#         input_ids = torch.cat([q_input_ids, a_input_ids, eos_tensor], dim=1)
#         labels = torch.cat(
#             [
#                 torch.full_like(q_input_ids, self.ignore_index),
#                 a_input_ids,
#                 eos_tensor,
#             ],
#             dim=1,
#         )
#         return input_ids, labels

#     def __call__(self, features: List[Tuple[str, str, str]]) -> Dict[str, torch.Tensor]:
#         input_ids_list = []
#         labels_list = []
#         pixel_values_list = []
#         max_length = 0

#         # TODO collatorçš„é—®é¢˜éœ€è¦æ’æŸ¥ä¸€ä¸‹ï¼Ÿè®­ç»ƒå‡ºæ¥çš„æ°¸è¿œéƒ½æ˜¯åŒä¸€ä¸ªç­”æ¡ˆ
#         # å¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰æ­£ç¡®å¤„ç†è¾“å…¥çš„tokenizationå’Œpadding

#         for query, answer, image_path in features:
#             messages = [
#                 {"role": "system", "content": self.system_prompt},
#                 {"role": "user", "content": query},
#             ]
#             try:
#                 prompt = self.processor.tokenizer.apply_chat_template(
#                     messages,
#                     tokenize=False,
#                     add_generation_prompt=True,
#                 )
#             except Exception:
#                 prompt = f"{self.system_prompt}\n{query}"

#             image = Image.open(image_path).convert("RGB")
#             self.processor.image_processor.patch_size = 14  # å¦‚æœæ¨¡å‹æ˜¯åŸºäº ViT æˆ–ç±»ä¼¼ç»“æ„
#             # print("Patch size:", self.processor.image_processor.patch_size)
#             model_inputs = self.processor(image, prompt, return_tensors="pt")
#             answer_ids = self.processor.tokenizer(
#                 answer,
#                 return_tensors="pt",
#                 max_length=2048,
#                 truncation=True,
#                 padding=False,
#             )["input_ids"]

#             input_ids, labels = self.convert_sample(
#                 model_inputs["input_ids"], answer_ids
#             )
#             input_ids_list.append(input_ids)
#             labels_list.append(labels)
#             pixel_values_list.append(model_inputs["pixel_values"])
#             max_length = max(max_length, input_ids.shape[1])

#         batch_input_ids = []
#         batch_labels = []

#         for input_ids, labels in zip(input_ids_list, labels_list):
#             pad_len = max_length - input_ids.shape[1]
#             pad_input_ids = torch.full(
#                 (1, pad_len), self.processor.tokenizer.pad_token_id, dtype=torch.long
#             )
#             pad_labels = torch.full((1, pad_len), self.ignore_index, dtype=torch.long)
#             batch_input_ids.append(torch.cat([pad_input_ids, input_ids], dim=1))
#             batch_labels.append(torch.cat([pad_labels, labels], dim=1))

#         batch = {
#             "input_ids": torch.cat(batch_input_ids, dim=0),
#             "labels": torch.cat(batch_labels, dim=0),
#             "pixel_values": torch.cat(pixel_values_list, dim=0),
#         }
#         batch["attention_mask"] = (
#             batch["input_ids"] != self.processor.tokenizer.pad_token_id
#         ).long()
#         return batch


# class TrainLlavaModelCollator:
#     def __init__(
#         self,
#         processor: AutoProcessor,
#         ignore_index: int = -100,
#         system_prompt: Optional[str] = None,
#         debug: bool = False,  # æ·»åŠ è°ƒè¯•æ ‡å¿—
#     ):
#         self.processor = processor
#         self.ignore_index = ignore_index
#         self.system_prompt = system_prompt or "You are a helpful assistant."
#         self.debug = debug
#         self.call_count = 0  # è®°å½•è°ƒç”¨æ¬¡æ•°
#         if (
#             not hasattr(self.processor, "patch_size")
#             or self.processor.patch_size is None
#         ):
#             self.processor.patch_size = 14

#     def convert_sample(
#         self,
#         q_input_ids: torch.Tensor,
#         a_input_ids: torch.Tensor,
#     ) -> Tuple[torch.Tensor, torch.Tensor]:
#         eos_token_id = self.processor.tokenizer.eos_token_id
#         eos_tensor = torch.tensor([[eos_token_id]], dtype=torch.long)
#         input_ids = torch.cat([q_input_ids, a_input_ids, eos_tensor], dim=1)
#         labels = torch.cat(
#             [
#                 torch.full_like(q_input_ids, self.ignore_index),
#                 a_input_ids,
#                 eos_tensor,
#             ],
#             dim=1,
#         )
#         return input_ids, labels

#     def __call__(self, features: List[Tuple[str, str, str]]) -> Dict[str, torch.Tensor]:
#         input_ids_list = []
#         labels_list = []
#         pixel_values_list = []
#         max_length = 0

#         for i, (query, answer, image_path) in enumerate(features):
#             prompt = f"{self.system_prompt}\n{query}"

#             if self.debug:
#                 print(f"Generated prompt: {prompt[:200]}...")

#             image = Image.open(image_path).convert("RGB")
#             self.processor.image_processor.patch_size = 14
#             model_inputs = self.processor(image, prompt, return_tensors="pt")
#             answer_ids = self.processor.tokenizer(
#                 answer,
#                 return_tensors="pt",
#                 max_length=2048,
#                 truncation=True,
#                 padding=True,
#             )["input_ids"]

#             if self.debug:
#                 print(f"Query tokens: {model_inputs['input_ids'].shape}")
#                 print(f"Answer tokens: {answer_ids.shape}")
#                 print(f"Query token ids: {model_inputs['input_ids'][0, :20]}")
#                 print(f"Answer token ids: {answer_ids[0, :20]}")

#                 # è§£ç éƒ¨åˆ†tokensæŸ¥çœ‹å†…å®¹
#                 query_text = self.processor.tokenizer.decode(
#                     model_inputs["input_ids"][0, :50]
#                 )
#                 answer_text = self.processor.tokenizer.decode(answer_ids[0, :50])
#                 print(f"Query decoded: {query_text}")
#                 print(f"Answer decoded: {answer_text}")

#             input_ids, labels = self.convert_sample(
#                 model_inputs["input_ids"], answer_ids
#             )
#             input_ids_list.append(input_ids)
#             labels_list.append(labels)
#             pixel_values_list.append(model_inputs["pixel_values"])
#             max_length = max(max_length, input_ids.shape[1])

#         batch_input_ids = []
#         batch_labels = []

#         for input_ids, labels in zip(input_ids_list, labels_list):
#             pad_len = max_length - input_ids.shape[1]
#             pad_input_ids = torch.full(
#                 (1, pad_len), self.processor.tokenizer.pad_token_id, dtype=torch.long
#             )
#             pad_labels = torch.full((1, pad_len), self.ignore_index, dtype=torch.long)
#             batch_input_ids.append(torch.cat([pad_input_ids, input_ids], dim=1))
#             batch_labels.append(torch.cat([pad_labels, labels], dim=1))

#         batch = {
#             "input_ids": torch.cat(batch_input_ids, dim=0),
#             "labels": torch.cat(batch_labels, dim=0),
#             "pixel_values": torch.cat(pixel_values_list, dim=0),
#         }
#         batch["attention_mask"] = (
#             batch["input_ids"] != self.processor.tokenizer.pad_token_id
#         ).long()
#         return batch

from typing import List, Tuple, Dict, Optional
import torch
from PIL import Image
from transformers import AutoProcessor


class TrainLlavaModelCollator:
    def __init__(
        self,
        processor: AutoProcessor,
        ignore_index: int = -100,
        system_prompt: Optional[str] = None,
        debug: bool = False,
    ):
        self.processor = processor
        self.ignore_index = ignore_index
        self.system_prompt = system_prompt or "You are a helpful assistant."
        self.debug = debug
        self.call_count = 0

        # è®¾ç½® patch size
        if (
            not hasattr(self.processor, "patch_size")
            or self.processor.patch_size is None
        ):
            self.processor.patch_size = 14

    def convert_sample(
        self,
        q_input_ids: torch.Tensor,
        a_input_ids: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        # ä¿è¯åœ¨åŒä¸€ device ä¸Šæ„é€  eos_tensor
        eos_token_id = self.processor.tokenizer.eos_token_id
        eos_tensor = torch.tensor(
            [eos_token_id], dtype=torch.long, device=q_input_ids.device
        ).unsqueeze(0)

        # æ£€æŸ¥ answer_ids æ˜¯å¦æœ‰éæ³• tokenï¼ˆå¦‚è¶…å‡º vocab èŒƒå›´ï¼‰
        assert (
            a_input_ids.max() < self.processor.tokenizer.vocab_size
        ), f"âŒ Token ID è¶…å‡º vocab èŒƒå›´ï¼šæœ€å¤§ {a_input_ids.max()} >= vocab_size={self.processor.tokenizer.vocab_size}"

        # æ‹¼æ¥è¾“å…¥å’Œæ ‡ç­¾ï¼ˆé—®å¥ä¸å‚ä¸ lossï¼‰
        input_ids = torch.cat([q_input_ids, a_input_ids, eos_tensor], dim=1)
        labels = torch.cat(
            [torch.full_like(q_input_ids, self.ignore_index), a_input_ids, eos_tensor],
            dim=1,
        )

        return input_ids, labels

    def __call__(self, features: List[Tuple[str, str, str]]) -> Dict[str, torch.Tensor]:
        input_ids_list = []
        labels_list = []
        pixel_values_list = []
        max_length = 0

        for i, (query, answer, image_path) in enumerate(features):
            prompt = f"{self.system_prompt}\n{query}"

            if self.debug:
                print(f"\nğŸ‘‰ [{i}] Prompt: {prompt[:100]}...")

            # å›¾åƒå¤„ç†
            image = Image.open(image_path).convert("RGB")
            self.processor.image_processor.patch_size = 14
            model_inputs = self.processor(image, prompt, return_tensors="pt")

            # æ–‡æœ¬å¤„ç†
            answer_ids = self.processor.tokenizer(
                answer,
                return_tensors="pt",
                max_length=2048,
                truncation=True,
                padding=True,
            )["input_ids"]

            if self.debug:
                print(f"â†’ query shape: {model_inputs['input_ids'].shape}")
                print(f"â†’ answer shape: {answer_ids.shape}")
                print(f"â†’ query tokens: {model_inputs['input_ids'][0, :10]}")
                print(f"â†’ answer tokens: {answer_ids[0, :10]}")
                print(
                    f"â†’ query decoded: {self.processor.tokenizer.decode(model_inputs['input_ids'][0, :50])}"
                )
                print(
                    f"â†’ answer decoded: {self.processor.tokenizer.decode(answer_ids[0, :50])}"
                )
                print(f"â†’ max token ID in answer: {answer_ids.max().item()}")

            # æ‹¼æ¥ input + label
            input_ids, labels = self.convert_sample(
                model_inputs["input_ids"], answer_ids
            )
            input_ids_list.append(input_ids)
            labels_list.append(labels)
            pixel_values_list.append(model_inputs["pixel_values"])
            max_length = max(max_length, input_ids.shape[1])

        # ç»Ÿä¸€ pad æˆ batch
        batch_input_ids = []
        batch_labels = []

        for input_ids, labels in zip(input_ids_list, labels_list):
            pad_len = max_length - input_ids.shape[1]

            pad_input_ids = torch.full(
                (1, pad_len),
                self.processor.tokenizer.pad_token_id,
                dtype=torch.long,
                device=input_ids.device,
            )
            pad_labels = torch.full(
                (1, pad_len), self.ignore_index, dtype=torch.long, device=labels.device
            )

            padded_input_ids = torch.cat([pad_input_ids, input_ids], dim=1)
            padded_labels = torch.cat([pad_labels, labels], dim=1)

            batch_input_ids.append(padded_input_ids)
            batch_labels.append(padded_labels)

            if self.debug:
                print(
                    f"âœ… Padded input shape: {padded_input_ids.shape}, labels shape: {padded_labels.shape}"
                )

        # æ„é€  batch è¿”å›
        batch = {
            "input_ids": torch.cat(batch_input_ids, dim=0),
            "labels": torch.cat(batch_labels, dim=0),
            "pixel_values": torch.cat(pixel_values_list, dim=0),
        }

        # Attention mask è‡ªåŠ¨ç”Ÿæˆ
        batch["attention_mask"] = (
            batch["input_ids"] != self.processor.tokenizer.pad_token_id
        ).long()

        return batch


def safe_cat(tensors, dim=0, pad_value=0):
    """
    è‡ªåŠ¨å¯¹é½é™¤æŒ‡å®šç»´åº¦å¤–çš„æ‰€æœ‰ç»´åº¦ï¼Œå¯¹è¾“å…¥ tensor åˆ—è¡¨è¿›è¡Œæ‹¼æ¥ã€‚
    """
    # è®¡ç®—ç›®æ ‡ shape
    max_shape = list(tensors[0].shape)
    for t in tensors[1:]:
        for i in range(len(max_shape)):
            if i != dim:
                max_shape[i] = max(max_shape[i], t.shape[i])

    padded = []
    for t in tensors:
        pad_dims = []
        for i in reversed(range(len(max_shape))):
            if i == dim:
                pad_dims.extend([0, 0])
            else:
                diff = max_shape[i] - t.shape[i]
                pad_dims.extend([0, diff])
        padded_tensor = F.pad(t, pad_dims, value=pad_value)
        padded.append(padded_tensor)

    return torch.cat(padded, dim=dim)


def main(is_train: bool = True):
    data_dir = "data/image-text-to-text/LIDC-IDRI-MLLM-CLF-EN"
    llavadataset = LIDCClassificationDataset(data_dir, is_train=is_train)
    print(len(llavadataset))
    item = llavadataset[0]
    print("human_input:", item[0])
    print("chatbot_output:", item[1])
    print("image_path:", item[2])

    # æµ‹è¯• TrainLlavaModelCollator
    print("\n=== æµ‹è¯• Collator ===")
    from transformers import AutoProcessor

    # åŠ è½½å¤„ç†å™¨ - ä½¿ç”¨å®é™…çš„LLaVAå¤„ç†å™¨ï¼Œä½ éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ¨¡å‹åç§°
    try:
        processor_name = "/home/qianq/model/llava-1.5-7b-hf"
        processor = AutoProcessor.from_pretrained(processor_name)

    except ImportError as e:
        print(f"åŠ è½½å¤„ç†å™¨å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–åº“ï¼Œä¾‹å¦‚: pip install transformers")
    except Exception as e:
        print(f"æµ‹è¯• collator æ—¶å‡ºé”™: {e}")
        traceback.print_exc()

    # TODO æŠŠæ‰€æœ‰çš„æ•°æ®éƒ½è·‘ä¸€éï¼Œå¹¶è§£ç labelsçœ‹ä¸‹ç»“æœï¼Œè°¢è°¢ï¼
    # from tqdm import trange
    from tqdm import tqdm

    print("\n=== éå†æ‰€æœ‰æ•°æ®å¹¶è§£ç labels ===")
    # åˆ›å»ºå¸¦è°ƒè¯•ä¿¡æ¯çš„collator
    debug_collator = TrainLlavaModelCollator(processor=processor)
    pad_token_id = (
        processor.tokenizer.pad_token_id
        if processor.tokenizer.pad_token_id is not None
        else 0
    )

    # åˆ†æ‰¹å¤„ç†æ•°æ®ï¼Œé¿å…å†…å­˜é—®é¢˜
    batch_size = 2
    total_samples = len(llavadataset)

    bar = tqdm(total=total_samples, desc="Processing batches", unit="batch")
    for start_idx in range(0, total_samples, batch_size):  # æˆ‘è¦æ‰€æœ‰çš„æ ·æœ¬
        bar.update(batch_size)
        end_idx = min(start_idx + batch_size, total_samples)

        # è·å–æ‰¹æ¬¡æ•°æ®
        batch_samples = [llavadataset[i] for i in range(start_idx, end_idx)]

        # æ‰“å°åŸå§‹æ•°æ®
        # print("\n--- åŸå§‹æ•°æ® ---")
        # for i, (human_input, chatbot_output, image_path) in enumerate(batch_samples):
        # print(f"æ ·æœ¬ {start_idx + i}:")
        # print(f"  è¾“å…¥: {human_input[:100]}...")
        # print(f"  æœŸæœ›è¾“å‡º: {chatbot_output[:100]}...")
        # print(f"  å›¾ç‰‡è·¯å¾„: {image_path}")
        # åº”ç”¨collatorå¤„ç†

        batch = debug_collator(batch_samples)

        # è¯¦ç»†åˆ†ælabels
        # print("\n--- Labelsåˆ†æ ---")
        labels = batch["labels"]
        querys = batch["input_ids"]
        # images = batch['pixel_values']
        # TODO å¯¹æ‰€æœ‰batchä¸­çš„imagesã€querysã€labelsæ•´åˆåˆ°ä¸€èµ·è¿›è¡Œuniqueåˆ†æï¼Œ åˆ†ææ€»å…±æœ‰å¤šå°‘æ•°æ®ï¼Œ æœ‰å¤šå°‘ä¸ªuniqueå€¼ï¼Œå¹¶å¯¹labelsè§£ç åçš„uniqueå€¼è¿›è¡Œåˆ†æ
        # æ”¶é›†æ‰€æœ‰æ•°æ®ç”¨äºuniqueåˆ†æ
        if start_idx == 0:
            # all_images = images
            all_querys = querys
            all_labels = labels
        else:
            try:

                # all_images = safe_cat([all_images, images], dim=0, pad_value=0)        # å›¾åƒä¸€èˆ¬ padding 0
                all_querys = safe_cat(
                    [all_querys, querys], dim=0, pad_value=pad_token_id
                )
                all_labels = safe_cat([all_labels, labels], dim=0, pad_value=-100)
            except Exception as e:
                # print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')
                # print(querys.shape, labels.shape)
                # print(all_querys.shape, all_labels.shape)
                # # tokenizerè§£ç å¹¶åˆ†æï¼š
                # for i in range(querys.shape[0]):
                #     print("Label shape:", labels.shape)
                #     print("Max token id:", labels[i, :].max())
                #     print("Min token id:", labels[i, :].min())
                #     print("Dtype:", labels.dtype)
                #     print("Label IDs:", labels[i, :])
                #     print(f"Query {i}: [{processor.tokenizer.decode(querys[i, :], skip_special_tokens=True)}]")
                #     print(f"Label {i}: [{processor.tokenizer.decode(labels[i, :], skip_special_tokens=True)}]")
                # print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')
                raise e

        # å¯¹æ¯ä¸ªæ ·æœ¬è§£ç labels
        for i in range(labels.shape[0]):
            sample_valid_labels = labels[i, :][
                labels[i, :] != debug_collator.ignore_index
            ]
            if len(sample_valid_labels) > 0:
                decoded_labels = processor.tokenizer.decode(
                    sample_valid_labels, skip_special_tokens=True
                )
                # print(f"æ ·æœ¬ {start_idx + i} è§£ç æ ‡ç­¾: {decoded_labels}")
            else:
                print(f"æ ·æœ¬ {start_idx + i} æ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾")

        # å¦‚æœæ˜¯æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼Œè¿›è¡Œæ•´ä½“uniqueåˆ†æ
        if end_idx >= total_samples:
            print(f"\n{'='*60}")
            print("æ•´ä½“æ•°æ®åˆ†æ")
            print(f"{'='*60}")

            print(f"æ€»æ ·æœ¬æ•°: {all_labels.shape[0]}")
            # print(f"æ€»å›¾ç‰‡æ•°: {all_images.shape[0]}")
            print(f"æ€»æŸ¥è¯¢æ•°: {all_querys.shape[0]}")

            # åˆ†ælabelsçš„uniqueå€¼
            all_valid_labels = all_labels[all_labels != debug_collator.ignore_index]
            unique_labels = torch.unique(all_valid_labels)
            print(f"æ‰€æœ‰æœ‰æ•ˆæ ‡ç­¾æ•°é‡: {len(all_valid_labels)}")
            print(f"uniqueæ ‡ç­¾å€¼æ•°é‡: {len(unique_labels)}")
            print(f"uniqueæ ‡ç­¾å€¼: {unique_labels}")

            # åˆ†ææ¯ä¸ªæ ·æœ¬çš„è§£ç æ ‡ç­¾
            decoded_labels_list = []
            for i in range(all_labels.shape[0]):
                sample_valid_labels = all_labels[i, :][
                    all_labels[i, :] != debug_collator.ignore_index
                ]
                if len(sample_valid_labels) > 0:
                    decoded_labels = processor.tokenizer.decode(
                        sample_valid_labels, skip_special_tokens=True
                    )
                    decoded_labels_list.append(decoded_labels)
                else:
                    decoded_labels_list.append("")

            # ç»Ÿè®¡uniqueè§£ç æ ‡ç­¾
            unique_decoded_labels = list(set(decoded_labels_list))
            print(f"uniqueè§£ç æ ‡ç­¾æ•°é‡: {len(unique_decoded_labels)}")
            print("uniqueè§£ç æ ‡ç­¾å†…å®¹:")
            for i, label in enumerate(unique_decoded_labels):
                print(f"  {i}: {label}")

            # ç»Ÿè®¡æ¯ä¸ªè§£ç æ ‡ç­¾çš„å‡ºç°æ¬¡æ•°
            from collections import Counter

            label_counts = Counter(decoded_labels_list)
            print("è§£ç æ ‡ç­¾å‡ºç°æ¬¡æ•°:")
            print(label_counts)

            # åˆ†æqueryçš„uniqueå€¼
            print(f"\n{'='*30} Queryåˆ†æ {'='*30}")
            all_valid_querys = all_querys[all_querys != pad_token_id]
            unique_querys = torch.unique(all_valid_querys)
            print(f"æ‰€æœ‰æœ‰æ•ˆæŸ¥è¯¢tokenæ•°é‡: {len(all_valid_querys)}")
            print(f"uniqueæŸ¥è¯¢tokenæ•°é‡: {len(unique_querys)}")
            print(f"uniqueæŸ¥è¯¢tokenå‰20ä¸ª: {unique_querys[:20]}")

            # åˆ†ææ¯ä¸ªæ ·æœ¬çš„è§£ç æŸ¥è¯¢
            decoded_querys_list = []
            for i in range(all_querys.shape[0]):
                sample_valid_querys = all_querys[i, :][all_querys[i, :] != pad_token_id]
                if len(sample_valid_querys) > 0:
                    decoded_querys = processor.tokenizer.decode(
                        sample_valid_querys, skip_special_tokens=True
                    )
                    decoded_querys_list.append(decoded_querys)
                else:
                    decoded_querys_list.append("")

            # ç»Ÿè®¡uniqueè§£ç æŸ¥è¯¢
            unique_decoded_querys = list(set(decoded_querys_list))
            print(f"uniqueè§£ç æŸ¥è¯¢æ•°é‡: {len(unique_decoded_querys)}")
            print("uniqueè§£ç æŸ¥è¯¢å†…å®¹:")
            for i, query in enumerate(unique_decoded_querys):
                print(f"  {i}: {query}...")

            # ç»Ÿè®¡æ¯ä¸ªè§£ç æŸ¥è¯¢çš„å‡ºç°æ¬¡æ•°
            query_counts = Counter(decoded_querys_list)
            print("è§£ç æŸ¥è¯¢å‡ºç°æ¬¡æ•°:")
            for query, count in query_counts.most_common(5):  # æ˜¾ç¤ºå‰5ä¸ªæœ€å¸¸è§çš„
                print(f"  å‡ºç°{count}æ¬¡: {query}...")


# def test_tokenize():
#     tokenizer = AutoProcessor.from_pretrained("/home/qianq/model/llava-1.5-7b-hf").tokenizer
#     text = "benign"
#     tokens = tokenizer(text, return_tensors="pt", padding="longest", truncation=True, max_length=2048)

if __name__ == "__main__":
    main(is_train=True)
    main(is_train=False)
